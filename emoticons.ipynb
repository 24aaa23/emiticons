{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of the Code\n",
        "\n",
        "This code implements a **Gradient Boosting Classifier** to classify emoticon sequences. It starts by loading the dataset and splits the data into features (`X`) and labels (`Y`). The code creates a unique mapping for each emoticon by assigning an integer to each unique character. The emoticon strings are then converted into lists of these integers and padded to ensure consistent sequence lengths across the dataset.\n",
        "\n",
        "The padded data is split into training and testing sets (80% for training, 20% for testing). The Gradient Boosting Classifier is trained on the training set, and predictions are made on the test set. Finally, the model's performance is evaluated using the accuracy score, which is printed as the final result.\n",
        "\n"
      ],
      "metadata": {
        "id": "e2jAQanrFl4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "emoticon_data = pd.read_csv(\"train_emoticon.csv\")\n",
        "\n",
        "# Split data into features (X) and target (Y)\n",
        "X = emoticon_data['input_emoticon']\n",
        "Y = emoticon_data['label']\n",
        "\n",
        "# Step 1: Create a unique mapping for each emoticon\n",
        "unique_emoticons = set(''.join(X))  # Create a set of all unique emoticons\n",
        "emoticon_to_num = {emoticon: idx for idx, emoticon in enumerate(unique_emoticons)}\n",
        "\n",
        "# Step 2: Convert each emoticon string to a list of numbers\n",
        "X_numeric = [[emoticon_to_num[emoticon] for emoticon in emoticon_string] for emoticon_string in X]\n",
        "\n",
        "# Step 3: Pad sequences manually using NumPy\n",
        "max_len = max(len(seq) for seq in X_numeric)  # Get the maximum sequence length\n",
        "X_padded = np.array([seq + [0] * (max_len - len(seq)) for seq in X_numeric])  # Pad with zeros\n",
        "\n",
        "# Step 4: Split the data into training and test sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_padded, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Initialize and train the Gradient Boosting model\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_model.fit(X_train, Y_train)\n",
        "# Step 6: Predict on the test set\n",
        "Y_pred = gb_model.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate the model\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(f\"Gradient Boosting Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "lKpW3SCdVsKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "612cface-aaf7-4d58-fdb1-e089e2371305"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Accuracy: 0.6158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRCROBNZw28K",
        "outputId": "41944325-bf6f-4292-b853-57df88489706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of the Code\n",
        "\n",
        "This code implements an **LSTM (Long Short-Term Memory) neural network** to classify emoticon sequences.\n",
        "\n",
        "- **Data Preparation**: The dataset is loaded, and a unique mapping of each emoticon to a numeric value is created. The sequences are then padded to ensure uniform length using Keras' `pad_sequences`. The target variable (`Y`) is label-encoded and converted to one-hot encoding for multi-class classification.\n",
        "\n",
        "- **Model Architecture**: The neural network uses an Embedding layer to transform the numeric values of the emoticons into dense vectors, followed by an LSTM layer to capture sequence dependencies. A Dropout layer is added to prevent overfitting, and the final Dense layer uses softmax for multi-class classification.\n",
        "\n",
        "- **Training and Evaluation**: The model is compiled using the Adam optimizer and categorical crossentropy as the loss function. It is trained over 10 epochs with a batch size of 32, and the test accuracy is evaluated. The model achieves an accuracy of 94.84%.\n",
        "\n"
      ],
      "metadata": {
        "id": "MQcjMyN8G_cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "emoticon_data = pd.read_csv(\"train_emoticon.csv\")\n",
        "\n",
        "# Step 1: Create a unique mapping for each emoticon\n",
        "X = emoticon_data['input_emoticon']\n",
        "Y = emoticon_data['label']\n",
        "\n",
        "# Create a unique mapping for emoticons\n",
        "unique_emoticons = set(''.join(X))  # Create a set of all unique emoticons\n",
        "emoticon_to_num = {emoticon: idx + 1 for idx, emoticon in enumerate(unique_emoticons)}  # Map emoticons to numbers, +1 for padding index\n",
        "\n",
        "# Convert each emoticon string to a list of numbers\n",
        "X_numeric = [[emoticon_to_num[emoticon] for emoticon in emoticon_string] for emoticon_string in X]\n",
        "\n",
        "# Step 2: Pad sequences manually using NumPy or Keras\n",
        "max_len = max(len(seq) for seq in X_numeric)  # Get the maximum sequence length\n",
        "X_padded = pad_sequences(X_numeric, maxlen=max_len, padding='post', value=0)  # Pad with zeros\n",
        "\n",
        "# Step 3: Label encode the target variable (Y)\n",
        "label_encoder = LabelEncoder()\n",
        "Y_encoded = label_encoder.fit_transform(Y)\n",
        "Y_categorical = to_categorical(Y_encoded)  # Convert to one-hot encoding if multi-class classification\n",
        "\n",
        "# Step 4: Split the data into training and test sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_padded, Y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Define the LSTM model\n",
        "vocab_size = len(emoticon_to_num) + 1  # +1 for padding\n",
        "embedding_dim = 64\n",
        "lstm_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Add Embedding layer (converts each emoticon number to a dense vector of given size)\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
        "\n",
        "# Add LSTM layer\n",
        "model.add(LSTM(lstm_units, return_sequences=False))\n",
        "\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Add a Dense output layer with softmax for classification\n",
        "model.add(Dense(Y_categorical.shape[1], activation='softmax'))  # Assuming multi-class classification\n",
        "\n",
        "# Step 6: Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 7: Train the model\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=batch_size, epochs=epochs, verbose=2)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, Y_test)\n",
        "print(f\"Neural Network Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuXpzz0fz55U",
        "outputId": "32ee0b8e-ff0f-485f-9585-4d9c5b9a6541"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "177/177 - 6s - 32ms/step - accuracy: 0.7209 - loss: 0.5223 - val_accuracy: 0.8715 - val_loss: 0.3139\n",
            "Epoch 2/10\n",
            "177/177 - 5s - 26ms/step - accuracy: 0.8895 - loss: 0.2525 - val_accuracy: 0.9075 - val_loss: 0.2299\n",
            "Epoch 3/10\n",
            "177/177 - 3s - 17ms/step - accuracy: 0.9190 - loss: 0.1905 - val_accuracy: 0.9336 - val_loss: 0.1731\n",
            "Epoch 4/10\n",
            "177/177 - 5s - 29ms/step - accuracy: 0.9340 - loss: 0.1569 - val_accuracy: 0.9463 - val_loss: 0.1356\n",
            "Epoch 5/10\n",
            "177/177 - 6s - 32ms/step - accuracy: 0.9499 - loss: 0.1219 - val_accuracy: 0.9400 - val_loss: 0.1439\n",
            "Epoch 6/10\n",
            "177/177 - 4s - 25ms/step - accuracy: 0.9522 - loss: 0.1151 - val_accuracy: 0.9435 - val_loss: 0.1389\n",
            "Epoch 7/10\n",
            "177/177 - 7s - 38ms/step - accuracy: 0.9605 - loss: 0.0960 - val_accuracy: 0.9477 - val_loss: 0.1189\n",
            "Epoch 8/10\n",
            "177/177 - 4s - 20ms/step - accuracy: 0.9684 - loss: 0.0810 - val_accuracy: 0.9541 - val_loss: 0.1030\n",
            "Epoch 9/10\n",
            "177/177 - 5s - 29ms/step - accuracy: 0.9723 - loss: 0.0714 - val_accuracy: 0.9484 - val_loss: 0.1310\n",
            "Epoch 10/10\n",
            "177/177 - 7s - 37ms/step - accuracy: 0.9721 - loss: 0.0710 - val_accuracy: 0.9484 - val_loss: 0.1105\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9508 - loss: 0.1069\n",
            "Neural Network Test Accuracy: 0.9484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of the Code\n",
        "\n",
        "This code implements an LSTM-based neural network for emoticon classification using a dataset of emoticons and their corresponding labels. The process starts by loading the dataset and converting each emoticon string into a numeric sequence. A unique numeric mapping is created for each emoticon, and the sequences are padded to ensure uniform length. The target labels are converted into a one-hot encoded format for multi-class classification.\n",
        "\n",
        "The data is split into two parts: 20% of the data is used for training, and 20% is reserved for testing. The model is built using a sequential architecture with an embedding layer that converts numeric emoticons into dense vectors. This is followed by an LSTM layer to capture temporal patterns, a dropout layer to prevent overfitting, and a final dense layer with softmax activation to predict the class of each input sequence.\n",
        "\n",
        "The model is compiled using the Adam optimizer and categorical cross-entropy loss function, and is trained over 10 epochs with a batch size of 32. After training, the model achieves a test accuracy of 86%, demonstrating a reasonable performance on the emoticon classification task.\n",
        "\n",
        "This pipeline showcases how LSTM networks can be effectively applied to sequence-based classification problems like emoticon recognition.\n"
      ],
      "metadata": {
        "id": "BeMgmwZ-HiC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "emoticon_data = pd.read_csv(\"train_emoticon.csv\")\n",
        "\n",
        "# Split data into features (X) and target (Y)\n",
        "X = emoticon_data['input_emoticon']\n",
        "Y = emoticon_data['label']\n",
        "\n",
        "# Step 1: Create a unique mapping for each emoticon\n",
        "unique_emoticons = set(''.join(X))  # Create a set of all unique emoticons\n",
        "emoticon_to_num = {emoticon: idx + 1 for idx, emoticon in enumerate(unique_emoticons)}  # +1 for padding\n",
        "\n",
        "# Step 2: Convert each emoticon string to a list of numbers\n",
        "X_numeric = [[emoticon_to_num[emoticon] for emoticon in emoticon_string] for emoticon_string in X]\n",
        "\n",
        "# Step 3: Pad sequences manually using NumPy\n",
        "max_len = max(len(seq) for seq in X_numeric)  # Get the maximum sequence length\n",
        "X_padded = np.array([seq + [0] * (max_len - len(seq)) for seq in X_numeric])  # Pad with zeros\n",
        "\n",
        "# Step 4: Convert labels to categorical format\n",
        "Y_categorical = to_categorical(Y)\n",
        "\n",
        "# Step 5: Split the data into 20% training and 20% testing\n",
        "# First, split off the 20% test set from the full data\n",
        "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X_padded, Y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Then, split off 25% of the remaining 80% for training (which is 20% of the total)\n",
        "X_train, _, Y_train, _ = train_test_split(X_train_val, Y_train_val, test_size=0.75, random_state=42)\n",
        "\n",
        "# Step 6: Define the LSTM model\n",
        "vocab_size = len(emoticon_to_num) + 1  # +1 for padding\n",
        "embedding_dim = 64\n",
        "lstm_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Add Embedding layer (converts each emoticon number to a dense vector of given size)\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
        "\n",
        "# Add LSTM layer\n",
        "model.add(LSTM(lstm_units, return_sequences=False))\n",
        "\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Add a Dense output layer with softmax for classification\n",
        "model.add(Dense(Y_categorical.shape[1], activation='softmax'))  # Assuming multi-class classification\n",
        "\n",
        "# Step 7: Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 8: Train the model on 20% training data\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=batch_size, epochs=epochs, verbose=2)\n",
        "\n",
        "# Step 9: Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, Y_test)\n",
        "print(f\"Neural Network Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyWlQYgS0T0l",
        "outputId": "9a41aafe-6a13-4906-d5a3-b5df4e8e40a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45/45 - 3s - 77ms/step - accuracy: 0.5388 - loss: 0.6913 - val_accuracy: 0.5078 - val_loss: 0.6883\n",
            "Epoch 2/10\n",
            "45/45 - 2s - 50ms/step - accuracy: 0.6716 - loss: 0.6019 - val_accuracy: 0.7754 - val_loss: 0.4875\n",
            "Epoch 3/10\n",
            "45/45 - 1s - 28ms/step - accuracy: 0.8404 - loss: 0.3713 - val_accuracy: 0.8319 - val_loss: 0.3721\n",
            "Epoch 4/10\n",
            "45/45 - 1s - 28ms/step - accuracy: 0.8905 - loss: 0.2650 - val_accuracy: 0.8496 - val_loss: 0.3626\n",
            "Epoch 5/10\n",
            "45/45 - 2s - 46ms/step - accuracy: 0.9202 - loss: 0.2288 - val_accuracy: 0.8545 - val_loss: 0.3256\n",
            "Epoch 6/10\n",
            "45/45 - 2s - 40ms/step - accuracy: 0.9371 - loss: 0.1843 - val_accuracy: 0.8637 - val_loss: 0.3247\n",
            "Epoch 7/10\n",
            "45/45 - 1s - 26ms/step - accuracy: 0.9308 - loss: 0.1778 - val_accuracy: 0.8757 - val_loss: 0.3328\n",
            "Epoch 8/10\n",
            "45/45 - 1s - 28ms/step - accuracy: 0.9484 - loss: 0.1349 - val_accuracy: 0.8672 - val_loss: 0.3713\n",
            "Epoch 9/10\n",
            "45/45 - 1s - 22ms/step - accuracy: 0.9527 - loss: 0.1314 - val_accuracy: 0.8517 - val_loss: 0.3857\n",
            "Epoch 10/10\n",
            "45/45 - 1s - 22ms/step - accuracy: 0.9513 - loss: 0.1282 - val_accuracy: 0.8658 - val_loss: 0.3408\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8831 - loss: 0.2955\n",
            "Neural Network Test Accuracy: 0.8658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p2Y65RsY1jVL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}